{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e7792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlretrieve\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60616294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(download_url):\n",
    "    response = urllib2.urlopen(download_url)\n",
    "    file = open(download_url, 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eaa7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "dict ={\n",
    "        \"page-url\" : row[0], # row[0] is the url of the page / pdf document\n",
    "        \"pdf-url\" : ,\n",
    "        \"paragraph\" : \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a360be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.csv', 'wb') as obj:\n",
    "    csv_reader = csv.reader(obj)\n",
    "    for row in csv_reader:\n",
    "        webUrl  = urllib.request.urlopen(row)\n",
    "        data = webUrl.read()\n",
    "        list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f961e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(list, indent = 4) # creating a json object\n",
    "with open(\"output.json\", \"w\") as outfile: # file with given third argument is created with .json extension to store output\n",
    "    outfile.write(json_object) # the list of dictionaries is written into this newly created json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab47c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pdf(url): #This function checks if the url is a pdf(Type-A) or a Type-B link\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url): # returns all the urls of a page in case of Type-B url\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_tag in soup.findAll(\"a\"): # function to get all the pdf links present in the page in a Type-B url\n",
    "    href = a_tag.attrs.get(\"href\")\n",
    "    if href == \"\" or href is None:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to encode characters from other languages to english (utf-8)\n",
    "from bs4 import BeautifulSoup\n",
    "markup = b\"<h1>xedxe5xecxf9</h1>\"\n",
    "soup = BeautifulSoup(markup, 'html.parser', from_encoding=\"iso-8859-8\")\n",
    "print(soup.h1)\n",
    "print(soup.original_encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
